{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ray RLlib - Lab 03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oJ6ZI3_DNkR"
      },
      "source": [
        "# Biblioteca de Algoritmos - Lab 03\n",
        "\n",
        "Nos últimos anos, muitas bibliotecas RL foram desenvolvidas. Essas bibliotecas foram projetadas para ter todas as ferramentas necessárias para implementar e testar agentes de Aprendizado por Reforço .\n",
        "\n",
        "Ainda assim, elas se diferem muito. É por isso que é importante escolher uma biblioteca que seja rápida, confiável e relevante para sua tarefa de RL. Do ponto de vista técnico, existem algumas coisas a se ter em mente ao considerar uma bilioteca para RL.\n",
        "\n",
        "- **Suporte para bibliotecas de aprendizado de máquina existentes:** Como o RL normalmente usa algoritmos baseados em gradiente para aprender e ajustar funções de política, você vai querer que ele suporte sua biblioteca favorita (Tensorflow, Keras, Pytorch, etc.)\n",
        "- **Escalabilidade:** RL é computacionalmente intensivo e ter a opção de executar de forma distribuída torna-se importante ao atacar ambientes complexos.\n",
        "- **Composibilidade:** Os algoritmos de RL normalmente envolvem simulações e muitos outros componentes. Você vai querer uma biblioteca que permita reutilizar componentes de algoritmos de RL, que seja compatível com várias estruturas de aprendizado profundo.\n",
        "\n",
        "[Aqui](https://docs.google.com/spreadsheets/d/1ZWhViAwCpRqupA5E_xFHSaBaaBZ1wAjO6PvmmEEpXGI/edit#gid=0) você consegue visualizar uma lista com algumas bibliotecas existentes.\n",
        "\n",
        "<img src=\"https://i1.wp.com/neptune.ai/wp-content/uploads/RL-tools.png?resize=1024%2C372&ssl=1\" width=500>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lV7pvJEIz7w"
      },
      "source": [
        "## Ray RLlib\n",
        "\n",
        "[Ray](https://docs.ray.io/en/latest/) é uma plataforma de execução distribuída que fornece bases para paralelismo e escalabilidade que são simples de usar e permitem que os programas Python sejam escalados em qualquer lugar, de um notebook a um grande cluster. Além disso, construída sobre o Ray, temos a [RLlib](https://docs.ray.io/en/latest/rllib.html), que fornece uma API unificada que pode ser aproveitada em uma ampla gama de aplicações.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1838/1*_bomm09XtiZfQ52Kfz9Ciw.png\" width=600>\n",
        "\n",
        "\n",
        "A RLlib foi projetada para oferecer suporte a várias estruturas de aprendizado profundo (TensorFlow e PyTorch) e pode ser acessada por meio de uma API Python simples. Atualmente, ela vem com uma [série de algoritmos RL](https://docs.ray.io/en/latest/rllib-algorithms.html#available-algorithms-overview).\n",
        "\n",
        "Em particular, a RLlib permite um desenvolvimento rápido porque torna mais fácil construir algoritmos RL escaláveis ​​por meio da reutilização e montagem de implementações existentes. A RLlib também permite que os desenvolvedores usem redes neurais criadas com várias estruturas de aprendizado profundo e se integra facilmente a simuladores de terceiros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE7inIG6QBNH"
      },
      "source": [
        "## Configuração"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYuZ0_Yfpf7_"
      },
      "source": [
        "Você precisará fazer uma cópia deste notebook em seu Google Drive antes de editar. Você pode fazer isso com **Arquivo → Salvar uma cópia no Drive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB7IKKg9peTw"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO84pMtApgxS"
      },
      "source": [
        "# Seu trabalho será armazenado em uma pasta chamada `minicurso_rl` por padrão \n",
        "# para evitar que o tempo limite da instância do Colab exclua suas edições\n",
        "\n",
        "DRIVE_PATH = \"/content/gdrive/MyDrive/minicurso_rl/lab03\"\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace(\"\\\\\", \"\")\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir -p $DRIVE_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSTA7tLoNFqP"
      },
      "source": [
        "# a versão do ray compatível com a implementação dos agentes disponibilizada é a 1.4.0\n",
        "!pip install 'aioredis==1.3.1' > /dev/null 2>&1 \n",
        "!pip install 'ray==1.4.0' > /dev/null 2>&1 \n",
        "!pip install 'ray[rllib]==1.4.0' > /dev/null 2>&1 \n",
        "!pip install 'ray[tune]==1.4.0' > /dev/null 2>&1 \n",
        "!pip install torch > /dev/null 2>&1 \n",
        "!pip install lz4 > /dev/null 2>&1 \n",
        "\n",
        "# Dependências necessárias para gravar os vídeos\n",
        "!apt-get install -y xvfb x11-utils > /dev/null 2>&1 \n",
        "!pip install pyvirtualdisplay==0.2.* > /dev/null 2>&1 \n",
        "\n",
        "# Ambiente da competição\n",
        "!pip install --upgrade ceia-soccer-twos > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWh9m8C7Oxhh"
      },
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/ -y\n",
        "! python -m atari_py.import_roms /content/ROM/ > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oI65YH-PQf3"
      },
      "source": [
        "# Inicializa uma instância de um display virtual\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUW4lSC6bwH"
      },
      "source": [
        "# Carrega a extensão do notebook TensorBoard\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnHhucGnYgl"
      },
      "source": [
        "## Ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTfl12C-nc4E"
      },
      "source": [
        "O OpenAI Gym possui um wrapper VideoRecorder que pode gravar um vídeo do ambiente em formato MP4. Abaixo iremos interagir no ambiente do [Carpole](https://gym.openai.com/envs/CartPole-v0/) executando ações aleatórias e gravar o resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa4MOX6cnvHs"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "environment_id = \"CartPole-v0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p8w770ymDCh"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "env = gym.make(environment_id)\n",
        "before_training = os.path.join(\n",
        "    DRIVE_PATH, \"{}_before_training.mp4\".format(environment_id)\n",
        ")\n",
        "print(before_training)\n",
        "\n",
        "video = VideoRecorder(env, before_training)\n",
        "env.reset()\n",
        "for i in range(200):\n",
        "  env.render()\n",
        "  video.capture_frame()\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "video.close()\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2tc6ubkoABb"
      },
      "source": [
        "O código acima salvou o arquivo de vídeo no seu Drive. Para exibi-lo no notebook, você precisa de uma função auxiliar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l4LijcTmVzH"
      },
      "source": [
        "from base64 import b64encode\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMlkQEe-oGgq"
      },
      "source": [
        "O código abaixo renderiza os resultados. Você deve obter um vídeo semelhante ao abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkozAc9hmXXR"
      },
      "source": [
        "from IPython.display import HTML\n",
        "html = render_mp4(before_training)\n",
        "HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3J1ST-oQdSs"
      },
      "source": [
        "## Treinando um agente de Aprendizado por Reforço"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTACYSZTS-Hw"
      },
      "source": [
        "Primeiro, vamos começar a executar o Ray em segundo plano. Executar um `ray.shutdown()` seguido por um `ray.init()` deve dar início às coisas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvSjf1qNNJ1a"
      },
      "source": [
        "import ray\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, include_dashboard=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pngYiIx_4Cu"
      },
      "source": [
        "### Basic Python API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qZAEP4jtj-G"
      },
      "source": [
        "Em alto nível, RLlib fornece uma classe Trainer que contém uma política para interação com o ambiente. Por meio da interface do Trainer, a política pode ser treinada, avaliada ou computar uma ação. \n",
        "\n",
        "Para cada algoritmo gostaríamos de configurar os parâmetros (taxa de aprendizado, tamanho da rede, tamanho do batch, etc.) de acordo com a nossa aplicação.  Para isso o Ray fornece dois níveis de paramêtros que podemos alterar. Primeiramente temos os parâmetros comuns a todos os algoritmos. Você pode conferir uma lista com os parâmetros disponíveis através desse [link](https://docs.ray.io/en/latest/rllib-training.html#common-parameters).\n",
        "\n",
        "E para cada [algoritmo disponível no ray](https://docs.ray.io/en/latest/rllib-algorithms.html#available-algorithms-overview) temos os parâmetros específicos. Na imagem abaixo podemos ver os parâmetros específicos para o algoritmo [Policy Gradient](https://docs.ray.io/en/latest/rllib-algorithms.html#policy-gradients).\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1yKJDJViHE_F9JH7NTQMYtQL3KLBJoJyk' width=\"500\" >\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I80be-2wrWZa"
      },
      "source": [
        "import ray\n",
        "import ray.rllib.agents.pg as pg\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "config = pg.DEFAULT_CONFIG.copy()\n",
        "config[\"num_gpus\"] = 0\n",
        "config[\"num_workers\"] = 1\n",
        "config[\"lr\"] = 0.0004\n",
        "config[\"framework\"] = \"torch\"\n",
        "\n",
        "trainer = pg.PGTrainer(config=config, env=environment_id)\n",
        "episodes = 1000\n",
        "\n",
        "for i in range(episodes):\n",
        "   # Executa uma iteração de treinamento da política com Policy Gradient (PG)\n",
        "   result = trainer.train()\n",
        "   print(pretty_print(result))\n",
        "\n",
        "   if i % 100 == 0:\n",
        "       checkpoint = trainer.save()\n",
        "       print(\"checkpoint saved at\", checkpoint)\n",
        "\n",
        "last_checkpoint = trainer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoJAWgZl2aNH"
      },
      "source": [
        "print(\"Last checkpoint saved at\", last_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROwwS65A5Lyr"
      },
      "source": [
        "Agora vamos criar outro vídeo, mas desta vez escolha a ação recomendada pelo modelo treinado em vez de agir aleatoriamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNKSZPSlx45x"
      },
      "source": [
        "trainer = pg.PGTrainer(config=config, env=environment_id)\n",
        "trainer.restore(last_checkpoint)\n",
        "\n",
        "after_training = os.path.join(\n",
        "    DRIVE_PATH, \"{}after_training_basic_api.mp4\".format(environment_id)\n",
        ")\n",
        "after_video = VideoRecorder(env, after_training)\n",
        "observation = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  env.render()\n",
        "  after_video.capture_frame()\n",
        "  action = trainer.compute_action(observation)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "after_video.close()\n",
        "env.close()\n",
        "html = render_mp4(after_training)\n",
        "HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dDW0sAiAPZU"
      },
      "source": [
        "### Usando ambiente ou modelos personalizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fve8QkZ_s_E"
      },
      "source": [
        "A API Python fornece a flexibilidade necessária para aplicar o RLlib a novos problemas. Você precisará usar esta API se desejar usar ambientes ou modelos personalizados com RLlib. Abaixo veremos um exemplo de um ambiente e um modelo customizado.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Para maiores informações veja em [APIs Python avançadas](https://docs.ray.io/en/latest/rllib-training.html#advanced-python-apis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVrDZfSobwCQ"
      },
      "source": [
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.agents import pg\n",
        "from ray.rllib.env.env_context import EnvContext\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "from ray.tune.logger import pretty_print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTZRCkkebwUj"
      },
      "source": [
        "class SimpleCorridor(gym.Env):\n",
        "    \"\"\"Exemplo de um ambiente personalizado em que você tem que andar por um \n",
        "    corredor. Você pode configurar o comprimento do corredor através da \n",
        "    configuração do ambiente.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EnvContext):\n",
        "        self.end_pos = config[\"corridor_length\"]\n",
        "        self.cur_pos = 0\n",
        "        self.action_space = Discrete(2)\n",
        "        self.observation_space = Box(\n",
        "            0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
        "        # Define a seed. É usado apenas para a recompensa final.\n",
        "        self.seed(config.worker_index * config.num_workers)\n",
        "\n",
        "    def reset(self):\n",
        "        self.cur_pos = 0\n",
        "        return [self.cur_pos]\n",
        "\n",
        "    def step(self, action):\n",
        "        assert action in [0, 1], action\n",
        "        if action == 0 and self.cur_pos > 0:\n",
        "            self.cur_pos -= 1\n",
        "        elif action == 1:\n",
        "            self.cur_pos += 1\n",
        "        done = self.cur_pos >= self.end_pos\n",
        "        # Produz uma recompensa aleatória quando atingirmos a meta.\n",
        "        return [self.cur_pos], \\\n",
        "            random.random() * 2 if done else -0.1, done, {}\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T87SrLPjbwc-"
      },
      "source": [
        "class TorchCustomModel(TorchModelV2, nn.Module):\n",
        "    \"\"\"Exemplo de um modelo personalizado PyTorch que apenas delega para uma \n",
        "    fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name):\n",
        "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
        "                              model_config, name)\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
        "                                       model_config, name)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
        "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
        "        return fc_out, []\n",
        "\n",
        "    def value_function(self):\n",
        "        return torch.reshape(self.torch_sub_model.value_function(), [-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDvJsitnbwmP"
      },
      "source": [
        "# Também pode registrar a função de criar um ambiente explicitamente com:\n",
        "# register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
        "\n",
        "# Registrar o modelo customizado\n",
        "ModelCatalog.register_custom_model(\n",
        "    \"my_model\", TorchCustomModel\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"env\": SimpleCorridor,  # ou \"corridor\" se registrado\n",
        "    \"env_config\": {\n",
        "        \"corridor_length\": 5,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"custom_model\": \"my_model\",\n",
        "        \"vf_share_layers\": True,\n",
        "    },\n",
        "    \"num_workers\": 1,  \n",
        "    \"framework\": \"torch\",\n",
        "}\n",
        "\n",
        "stop = {\n",
        "    \"training_iteration\": 50,\n",
        "    \"timesteps_total\": 100000,\n",
        "    \"episode_reward_mean\": 0.1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhWUnHcvdWer"
      },
      "source": [
        "pg_config = pg.DEFAULT_CONFIG.copy()\n",
        "pg_config.update(config)\n",
        "pg_config[\"lr\"] = 1e-3\n",
        "\n",
        "trainer = pg.PGTrainer(config=pg_config, env=SimpleCorridor)\n",
        "# executa o loop de treinamento manual e imprime os resultados após cada iteração\n",
        "for _ in range(stop[\"training_iteration\"]):\n",
        "    result = trainer.train()\n",
        "    print(pretty_print(result))\n",
        "    \n",
        "    # pare o treinamento caso tiver alcançado a quantidade de steps desejada\n",
        "    # ou caso a recompensa desejada seja alcançada\n",
        "    if result[\"timesteps_total\"] >= stop[\"timesteps_total\"] or \\\n",
        "            result[\"episode_reward_mean\"] >= stop[\"episode_reward_mean\"]:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbljfr6_AbVD"
      },
      "source": [
        "### Ray Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpju4jhv0rQ"
      },
      "source": [
        "Todos os Trainers do RLlib são compatíveis com a API do [Ray Tune](https://docs.ray.io/en/master/tune/index.html). Isso permite que eles sejam facilmente usados em experimentos com o Tune. Por exemplo, o código a seguir executa o mesmo treino com o CartPole com o algoritmo PG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzscZjhAkHzm"
      },
      "source": [
        "import ray\n",
        "config = {\n",
        "    \"env\": environment_id,\n",
        "    \"framework\": \"torch\",\n",
        "}\n",
        "stop = {\"episode_reward_mean\": 150, \"timesteps_total\": 100000}\n",
        "\n",
        "# Executar o treinamento\n",
        "analysis = ray.tune.run(\n",
        "    \"PG\",\n",
        "    config=config,\n",
        "    stop=stop,\n",
        "    checkpoint_freq=10,\n",
        "    checkpoint_at_end=True,\n",
        "    local_dir=os.path.join(DRIVE_PATH, \"results\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WzUQbAopUd"
      },
      "source": [
        "Embora o objeto de análise retornado do `ray.tune.run` anteriormente não tivesse nenhuma instância Trainer, ele tem todas as informações necessárias para reconstruir um de um checkpoint salvo.\n",
        "\n",
        "O retorno do Ray Tune é um objeto [ExperimentAnalysis](https://docs.ray.io/en/latest/tune/api_docs/analysis.html?highlight=ExperimentAnalysis#experimentanalysis-tune-experimentanalysis) onde é possível resgatar qual o melhor checkpoint do treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO9YPVLVQuYo"
      },
      "source": [
        "from ray.rllib.agents.pg import PGTrainer\n",
        "\n",
        "# restaurar um Trainer \n",
        "trial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
        "checkpoint = analysis.get_best_checkpoint(\n",
        "  trial,\n",
        "  \"training_iteration\",\n",
        "  \"max\",\n",
        ")\n",
        "trainer = PGTrainer(config=config)\n",
        "trainer.restore(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657GIZg_pKGN"
      },
      "source": [
        "Agora vamos criar outro vídeo, mas desta vez escolha a ação recomendada pelo modelo treinado com a API Tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SB5Dw6blhl1"
      },
      "source": [
        "after_training = after_training = os.path.join(\n",
        "    DRIVE_PATH, \"{}after_training_tune.mp4\".format(environment_id)\n",
        ")\n",
        "after_video = VideoRecorder(env, after_training)\n",
        "observation = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  env.render()\n",
        "  after_video.capture_frame()\n",
        "  action = trainer.compute_action(observation)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "after_video.close()\n",
        "env.close()\n",
        "# You should get a video similar to the one below. \n",
        "html = render_mp4(after_training)\n",
        "HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoJFAzlJ9_kB"
      },
      "source": [
        "O Tune gera arquivos do [Tensorboard](https://www.tensorflow.org/tensorboard) automaticamente durante o `tune.run()` Para visualizar a aprendizagem no tensorboard, execute o célula abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3M56bN6E7l"
      },
      "source": [
        "%tensorboard --logdir /content/gdrive/MyDrive/minicurso_rl/lab03/results/PG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbjmesCGQjfa"
      },
      "source": [
        "## Hyperparameter Tuning com o Ray Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DkMG6VHmwHf"
      },
      "source": [
        "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) é uma biblioteca para execução de experimentos e ajuste de hiperparâmetros. Vamos agora tentar encontrar hiperparâmetros que possam resolver o ambiente [Cartpole](https://gym.openai.com/envs/CartPole-v1/) no menor número de passos de tempo. Esteja preparado para que demore um pouco para ser executado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8heIjMxdQtzb"
      },
      "source": [
        "parameter_search_config = {\n",
        "    \"env\": environment_id,\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 1,  # porcentagem da gpu disponível para treino\n",
        "    \"num_workers\": 1,  # número de workers além do processo principal; no colab deve ser 1 pois só há 2 CPUs\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    \"model\": {\n",
        "      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n",
        "      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n",
        "    },\n",
        "    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n",
        "}\n",
        "\n",
        "# To explicitly stop or restart Ray, use the shutdown API.\n",
        "ray.shutdown()\n",
        "\n",
        "ray.init(\n",
        "  num_cpus=2,\n",
        "  include_dashboard=False,\n",
        "  ignore_reinit_error=True,\n",
        "  log_to_driver=False,\n",
        ")\n",
        "\n",
        "parameter_search_analysis = ray.tune.run(\n",
        "  \"PG\",\n",
        "  config=parameter_search_config,\n",
        "  stop=stop,\n",
        "  num_samples=5,\n",
        "  metric=\"timesteps_total\",\n",
        "  mode=\"min\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE9G98MTm5Sf"
      },
      "source": [
        "print(\n",
        "  \"Melhores hiperparâmetros encontrados:\",\n",
        "  parameter_search_analysis.best_config,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257s7UgKxLwM"
      },
      "source": [
        "Especificando num_samples = 5 significa que você obterá cinco amostras aleatórias para a taxa de aprendizagem. Para cada um deles, existem dois valores para o tamanho da camada oculta e dois valores para a função de ativação. Portanto, haverá 5 * 2 * 2 = 20 tentativas, mostradas com seus status na saída da célula à medida que o cálculo é executado.\n",
        "\n",
        "Observe que Ray mostra a melhor configuração atual à medida que avança. Isso inclui todos os valores padrão que foram definidos, o que é um bom lugar para encontrar outros parâmetros que podem ser ajustados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpwtv91SBRfZ"
      },
      "source": [
        "## Exercício"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmaO8FQEBHas"
      },
      "source": [
        "Agora que você conhece a API básica do Ray Tune e da RLLib, **utilize o ambiente `BreakoutNoFrameskip-v4` e treine agentes com os algoritmos A3C, PPO e SAC**. Lembre-se de utilizar também o tensorboard para acompanhar e comparar as curvas de aprendizado de suas execuções.\n",
        "\n",
        "Descrições dos algoritmos e seus respectivos hiperparâmetros podem ser encontrados [aqui](https://docs.ray.io/en/latest/rllib-algorithms.html#available-algorithms-overview)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdfZ29RMCoRZ"
      },
      "source": [
        "# INSIRA AQUI O CÓDIGO PARA TREINAMENTO SOBRE O BreakoutNoFrameskip-v4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm_F9Zsb60fi"
      },
      "source": [
        "# Bônus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJm60-Zu69eH"
      },
      "source": [
        "Como tarefa bônus, experimente com os algoritmos aprendidos no ambiente `soccer_twos`, que será utilizado na competição final deste curso*. Para facilitar, utilize a variação `team_vs_policy` como no laboratório anterior.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
        "\n",
        "> Visualização do ambiente\n",
        "\n",
        "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [no repositório](https://github.com/bryanoliveira/soccer-twos-env) e [na documentação do Unity ml-agents](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
        "\n",
        "\n",
        "**Sua tarefa é treinar um agente com a interface do Ray apresentada, experimentando com diferentes algoritmos e hiperparâmetros.**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ORYtmYf823f"
      },
      "source": [
        "Utilize o ambiente instanciado abaixo para executar o algoritmo de treinamento. Ao final da execução, a recompensa do seu agente por episódio deve tender a +2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwotOfEt81mq"
      },
      "source": [
        "import soccer_twos\n",
        "\n",
        "# Fecha o ambiente caso tenha sido aberto anteriormente\n",
        "try: env.close()\n",
        "except: pass\n",
        "\n",
        "env = soccer_twos.make(\n",
        "    variation=soccer_twos.EnvType.team_vs_policy,\n",
        "    flatten_branched=True, # converte o action_space de MultiDiscrete para Discrete\n",
        "    single_player=True, # controla um dos jogadores enquanto os outros ficam parados\n",
        "    opponent_policy=lambda *_: 0,  # faz os oponentes ficarem parados\n",
        ")\n",
        "\n",
        "# Obtem tamanhos de estado e ação\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(\"Tamanho do estado: {}, tamanho da ação: {}\".format(state_size, action_size))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWfhclM8-xRy"
      },
      "source": [
        "from ray import tune\n",
        "\n",
        "def create_rllib_env(env_config: dict = {}):\n",
        "    # suporte a múltiplas instâncias do ambiente na mesma máquina\n",
        "    if hasattr(env_config, \"worker_index\"):\n",
        "        env_config[\"worker_id\"] = (\n",
        "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
        "            + env_config.vector_index\n",
        "        )\n",
        "    return soccer_twos.make(**env_config)\n",
        "\n",
        "# registra ambiente no Ray\n",
        "tune.registry.register_env(\"Soccer\", create_rllib_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsrDqXA31mt"
      },
      "source": [
        "Utilize a configuração abaixo como ponto de partida para seus testes. \n",
        "\n",
        "A parte mais imporante é a chave `env_config`, que configura o ambiente para ser compatível com o agente disponibilizado para exportação do seu agente. Neste ponto do curso você já deve conseguir testar as outras variações do ambiente e utilizar as APIs do Ray para treinar um agente próximo (ou melhor) do que o [ceia_baseline_agent](https://drive.google.com/file/d/1WEjr48D7QG9uVy1tf4GJAZTpimHtINzE/view). Exemplos de como utilizar as outras variações podem ser encontrados [aqui](https://github.com/dlb-rl/rl-tournament-starter/). Ao utilizar essas variações, você deve utilizar também outras definições de agente para lidar com os diferentes espaços de observação e ação (que também estão presentes nos exemplos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU6FFjY3-vsS"
      },
      "source": [
        "NUM_ENVS_PER_WORKER = 2\n",
        "\n",
        "analysis = tune.run(\n",
        "    \"PPO\",\n",
        "    config={\n",
        "        # system settings\n",
        "        \"num_gpus\": 1,\n",
        "        \"num_workers\": 1,\n",
        "        \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
        "        \"log_level\": \"INFO\",\n",
        "        \"framework\": \"torch\",\n",
        "        # RL setup\n",
        "        \"env\": \"Soccer\",\n",
        "        \"env_config\": {\n",
        "            \"num_envs_per_worker\": NUM_ENVS_PER_WORKER,\n",
        "            \"variation\": soccer_twos.EnvType.team_vs_policy,\n",
        "            \"single_player\": True,\n",
        "            \"flatten_branched\": True,\n",
        "        },\n",
        "    },\n",
        "    stop={\n",
        "        # 10000000 (10M) de steps podem ser necessários para aprender uma política útil\n",
        "        \"timesteps_total\": 10000000,\n",
        "        # você também pode limitar por tempo, de acordo com o tempo limite do colab\n",
        "        \"time_total_s\": 14400, # 4h\n",
        "    },\n",
        "    checkpoint_freq=100,\n",
        "    checkpoint_at_end=True,\n",
        "    local_dir=os.path.join(DRIVE_PATH, \"results\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmnTZpqAPiEE"
      },
      "source": [
        "## Exportando seu agente treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgLarb2Qp4A"
      },
      "source": [
        "Assim como no Lab 02, você pode exportar seu agente treinado para ser executado como competidor no ambiente da competição ou simplesmente assistí-lo. Para isso, devemos definir uma classe de agente que implemente a interface e trate as observações/ações para o formato da competição. Abaixo, configuramos qual experimento/checkpoint exportar e guardamos a implementação em uma variável para salvá-la em um arquivo posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4avcVx4RMpE"
      },
      "source": [
        "ALGORITHM = \"PPO\"\n",
        "TRIAL = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
        "CHECKPOINT = analysis.get_best_checkpoint(\n",
        "  TRIAL,\n",
        "  \"training_iteration\",\n",
        "  \"max\",\n",
        ")\n",
        "TRIAL, CHECKPOINT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnppiHT6Q5gK"
      },
      "source": [
        "agent_file = f\"\"\"\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import gym\n",
        "from gym_unity.envs import ActionFlattener\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.registry import get_trainable_cls\n",
        "\n",
        "from soccer_twos import AgentInterface, DummyEnv\n",
        "\n",
        "\n",
        "ALGORITHM = \"{ALGORITHM}\"\n",
        "CHECKPOINT_PATH = os.path.join(\n",
        "    os.path.dirname(os.path.abspath(__file__)), \n",
        "    \"{CHECKPOINT.split(\"lab03/\")[1]}\"\n",
        ")\n",
        "\n",
        "\n",
        "class MyRaySoccerAgent(AgentInterface):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        super().__init__()\n",
        "        ray.init(ignore_reinit_error=True)\n",
        "\n",
        "        self.flattener = ActionFlattener(env.action_space.nvec)\n",
        "\n",
        "        # Load configuration from checkpoint file.\n",
        "        config_path = \"\"\n",
        "        if CHECKPOINT_PATH:\n",
        "            config_dir = os.path.dirname(CHECKPOINT_PATH)\n",
        "            config_path = os.path.join(config_dir, \"params.pkl\")\n",
        "            # Try parent directory.\n",
        "            if not os.path.exists(config_path):\n",
        "                config_path = os.path.join(config_dir, \"../params.pkl\")\n",
        "\n",
        "        # Load the config from pickled.\n",
        "        if os.path.exists(config_path):\n",
        "            with open(config_path, \"rb\") as f:\n",
        "                config = pickle.load(f)\n",
        "        else:\n",
        "            # If no config in given checkpoint -> Error.\n",
        "            raise ValueError(\n",
        "                \"Could not find params.pkl in either the checkpoint dir or \"\n",
        "                \"its parent directory!\"\n",
        "            )\n",
        "\n",
        "        # no need for parallelism on evaluation\n",
        "        config[\"num_workers\"] = 0\n",
        "        config[\"num_gpus\"] = 0\n",
        "\n",
        "        # create a dummy env since it's required but we only care about the policy\n",
        "        obs_space = env.observation_space\n",
        "        act_space = self.flattener.action_space\n",
        "        tune.registry.register_env(\n",
        "            \"DummyEnv\",\n",
        "            lambda *_: DummyEnv(obs_space, act_space),\n",
        "        )\n",
        "        config[\"env\"] = \"DummyEnv\"\n",
        "\n",
        "        # create the Trainer from config\n",
        "        cls = get_trainable_cls(ALGORITHM)\n",
        "        agent = cls(env=config[\"env\"], config=config)\n",
        "        # load state from checkpoint\n",
        "        agent.restore(CHECKPOINT_PATH)\n",
        "        # get default policy for evaluation\n",
        "        self.policy = agent.get_policy()\n",
        "\n",
        "    def act(self, observation):\n",
        "        actions = {{}}\n",
        "        for player_id in observation:\n",
        "            # compute_single_action returns a tuple of (action, action_info, ...)\n",
        "            # as we only need the action, we discard the other elements\n",
        "            actions[player_id] = self.flattener.lookup_action(\n",
        "                self.policy.compute_single_action(observation[player_id])[0]\n",
        "            )\n",
        "        return actions\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yo3UPK8Pjsg"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "agent_name = \"my_ray_soccer_agent\"\n",
        "agent_path = os.path.join(DRIVE_PATH, agent_name, agent_name)\n",
        "shutil.rmtree(agent_path)\n",
        "os.makedirs(agent_path)\n",
        "\n",
        "# salva a classe do agente\n",
        "with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
        "    f.write(agent_file)\n",
        "\n",
        "# salva um __init__ para criar o módulo Python\n",
        "with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
        "    f.write(\"from .agent import MyRaySoccerAgent\")\n",
        "\n",
        "# copia o trial inteiro, incluindo os arquivos de configuração do experimento\n",
        "shutil.copytree(TRIAL, os.path.join(agent_path, TRIAL.split(\"lab03/\")[1]))\n",
        "\n",
        "# empacota tudo num arquivo .zip\n",
        "shutil.make_archive(os.path.join(DRIVE_PATH, agent_name), \"zip\", os.path.join(DRIVE_PATH, agent_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7DM2J46OFF"
      },
      "source": [
        "Após empacotar todos os arquivos necessários para a execução do seu agente, será criado um arquivo `minicurso_rl/lab03/my_ray_soccer_agent.zip` nos arquivos do Colab e na pasta correspondente no Google Drive. Baixe o arquivo e extraia-o para alguma pasta no seu computador. \n",
        "\n",
        "Assumindo que o ambiente Python já está configurado (e.g. os pacotes no [requirements.txt](https://github.com/dlb-rl/rl-tournament-starter/blob/main/requirements.txt) estão instalados), rode `python -m soccer_twos.watch -m my_ray_soccer_agent` para assistir seu agente jogando contra si mesmo. \n",
        "\n",
        "Você também pode testar dois agentes diferentes jogando um contra o outro. Utilize o seguinte comando: `python -m soccer_twos.watch -m1 my_ray_soccer_agent -m2 ceia_baseline_agent`. Você pode baixar o agente *ceia_baseline_agent* [aqui](https://drive.google.com/file/d/1WEjr48D7QG9uVy1tf4GJAZTpimHtINzE/view)."
      ]
    }
  ]
}